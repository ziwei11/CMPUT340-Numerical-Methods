{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Lab3_Least_Squares.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPUm01SvWQTC"
      },
      "source": [
        "<h2>Lab 3: Least Squares</h2>\n",
        "\n",
        "<b>Demo Date: </b> Oct. 13 <br>\n",
        "<b>Due Date: </b> Oct. 15\n",
        "\n",
        "In this lab you will implement two algorithms for solving an ill-conditioned least squares problem. We create an artificial and overdetermined least-squares problem by removing two columns of a $10 \\times 10$ Hilbert matrix, a classic ill-conditioned matrix. As we remove the two columns from the matrix it is no longer a Hilbert matrix, but it creates an overdetermined system with a large condition number: $\\approx 3,796,554,168$.\n",
        "\n",
        "Implement an algorithm that you believe will compute the value of $x$ for the least squares problem $Ax \\approx b$ as accurately as the problem allows. You will also implement an algorithm that you believe to produce inaccurate solutions due to the large condition number of the system. In this lab you do not need to worry about having a fast implementation, any non-vectorized implementation will be enough. \n",
        "\n",
        "Compare the two solutions in terms of the norm of their residual vector $Ax - b$. In the demo of your lab you should be able to explain your choices of algorithms and the results you obtained. Why did one method produce a solution with a smaller residual than the other? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Obd0lwnWQTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c763f513-5fe1-4521-9424-762150178ca2"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "from scipy.linalg import hilbert\n",
        "from scipy.optimize import lsq_linear\n",
        "\n",
        "\n",
        "# library method\n",
        "def lsq_v4_library(A, b):\n",
        "  x = lsq_linear(A, b)\n",
        "  return x.x\n",
        "\n",
        "\n",
        "# Normal equation\n",
        "# In theory the system of normal equations gives the exact solution to\n",
        "# a linear least square problem, but in practice this approach sometimes \n",
        "# yields disappointingly inaccurate results. There are two reasons for this behavior: \n",
        "# 1. Information can be lost in forming the cross-product matrix and right-hand-side vector.\n",
        "# 2. The condition number of the cross-product matrix, which determines the sensitivity of \n",
        "# the solution to the normal equations, is the square of that of the original matrix A.\n",
        "# There is a potential condition-squaring effect in the sensitive of least square solutions, \n",
        "# but this should be a significant factor only when the residual is large.\n",
        "# Unfortunately, the normal equation can suffer a condition-squaring effect even when the fit is good \n",
        "# and the residual is small, making the computed solution more sensitive than warranted by the underlying least squares problem.\n",
        "# In this sense, the normal equations method is unstable.\n",
        "def lsq_v3_normal(A, b):\n",
        "  b = b.reshape((len(b), 1))\n",
        "  # At_A * x = At_b\n",
        "  At = np.transpose(A)\n",
        "  At_A = np.dot(At, A)\n",
        "  At_b = np.dot(At, b)\n",
        "  # We use LU factorization to solve this here\n",
        "  L, U = lu_factor(copy.deepcopy(At_A))\n",
        "  y = forward_substituion(L, At_b)\n",
        "  x = back_substituion(U, y)     \n",
        "  return x\n",
        "\n",
        "\n",
        "def forward_substituion(L, b):\n",
        "  # change the data type of b\n",
        "  n = len(L)\n",
        "  x = np.zeros((L.shape[1], b.shape[1]))\n",
        "  for j in range(0, n):\n",
        "    if L[j][j] == 0:\n",
        "      break # singular matrix\n",
        "    x[j] = b[j] / L[j][j]\n",
        "\n",
        "    for i in range(j, n):\n",
        "      b[i] = b[i] - L[i][j] * x[j]\n",
        "  return x\n",
        "\n",
        "\n",
        "def back_substituion(U, b):\n",
        "  # change the data type of b\n",
        "  n = len(U)\n",
        "  x = np.zeros((U.shape[1], b.shape[1]))\n",
        "  for j in range(n - 1, -1, -1):\n",
        "    if U[j][j] == 0:\n",
        "      break # singular matrix\n",
        "    x[j] = b[j] / U[j][j]\n",
        "\n",
        "    for i in range(0, j):\n",
        "      b[i] = b[i] - U[i][j] * x[j]\n",
        "  return x\n",
        "\n",
        "\n",
        "def lu_factor(A):\n",
        "  # Another way to do this part\n",
        "  # change the datatype of A\n",
        "  A = A + 0.0\n",
        "  n = len(A)\n",
        "  U = copy.deepcopy(A)\n",
        "  L = np.eye(n, k = 0)\n",
        "  # iterate from the first column to the column before the last column\n",
        "  for k in range(0, n - 1):\n",
        "    if A[k][k] == 0:\n",
        "      break\n",
        "    # create a len(A) * len(A) identity matrix\n",
        "    # I = np.eye(len(A), k = 0, dtype = float)\n",
        "    I = np.eye(n, k = 0)\n",
        "    # create an e_k which is the transpose of the kth column of the identity matrix\n",
        "    e_k = np.zeros(shape = (1, n))\n",
        "    e_k[0][k] = 1\n",
        "    # create a m which is the kth column of the matrix U\n",
        "    m = copy.deepcopy(U[:, k])\n",
        "    m = m.reshape(n, 1)\n",
        "    # m_i = a_i / a_k, i = k + 1, ...\n",
        "    m = np.divide(m, U[k][k])\n",
        "    # set the values before m_k+1 equal to 0\n",
        "    m[0:(k + 1), 0] = 0\n",
        "    # M = I - m * e_k\n",
        "    M = I - np.dot(m, e_k)\n",
        "    # L = I + m * e_k\n",
        "    l = I + np.dot(m, e_k)\n",
        "    # M_n * ... * M_2 * M_1 * A = U\n",
        "    U = np.dot(M, U)\n",
        "    # L_1 * L_2 * ... * L_n = L\n",
        "    L = np.dot(L, l)\n",
        "\n",
        "  return L, U\n",
        "\n",
        "\n",
        "# Modified Gram-Schmidt\n",
        "# Modified Gram-Schmidt is equivalent mathematically, but superior numerically,\n",
        "# to classical Gram-Schmidt. Unlike either of the column-oriented Gram-Schmidt procedures, \n",
        "# the roworiented modified Gram-Schmidt procedure permits the use of column pivoting to identify \n",
        "# a maximal linearly independent set of columns of A.\n",
        "def lsq_v1_MGS(A, b):\n",
        "  b = b.reshape((len(b), 1))\n",
        "  m = A.shape[0]\n",
        "  n = A.shape[1]\n",
        "  # Create two matrix Q and R\n",
        "  R = np.zeros((n, n))\n",
        "  Q = np.zeros((m, n))\n",
        "  for k in range(0, n):        # loop over columns\n",
        "    R[k][k] = np.linalg.norm(A[:, k], ord = 2)\n",
        "    if R[k][k] == 0:         # stop if linearly dependent\n",
        "      break\n",
        "    Q[:, k] = A[:, k] /  R[k][k]       \n",
        "    for j in range(k + 1, n):        # substract from succeeding columns there components in current column\n",
        "      Qk_t = np.transpose(Q[:, k])\n",
        "      R[k][j] = np.dot(Qk_t, A[:, j])\n",
        "      A[:, j] = A[:, j] - R[k][j] * Q[:, k]\n",
        "  # c1 = Q^T * b\n",
        "  Q1_t = np.transpose(Q)\n",
        "  c1 = np.dot(Q1_t, b)\n",
        "  x = back_substituion(R, c1)\n",
        "  return x\n",
        "\n",
        "\n",
        "# Classic Gram-Schmidt\n",
        "# The classical Gram-Schmidt procedure is less than satisfactory when implemented \n",
        "# in finite-precision arithmetic, as orthogonality among the computed qk tends to be \n",
        "# lost due to rounding error. Moreover, the classical Gram-Schmidt procedure requires \n",
        "# separate storage for A and Q1 (as well as R) because the original ak is used in the \n",
        "# inner loop, and hence qx, which is updated in the inner loop, cannot overwrite it. \n",
        "def lsq_v5_CGS(A, b):\n",
        "  b = b.reshape((len(b), 1))\n",
        "  m = A.shape[0]\n",
        "  n = A.shape[1]\n",
        "  # Create two matrix Q and R\n",
        "  R = np.zeros((n, n))\n",
        "  Q = np.zeros((m, n))\n",
        "  for k in range(0, n):    # loop over columns\n",
        "    Q[:, k] = A[:, k]\n",
        "    for j in range(0, k):         # substract from current column its components in preceding columns\n",
        "      Qj_t = np.transpose(Q[:, j])\n",
        "      R[j][k] = np.dot(Qj_t, A[:, k])\n",
        "      Q[:, k] = Q[:, k] - R[j][k] * Q[:, j]\n",
        "    R[k][k] = np.linalg.norm(Q[:, k], ord = 2)\n",
        "    if R[k][k] == 0:      # stop if linearly dependent\n",
        "      break\n",
        "    Q[:, k] = Q[:, k] / R[k][k]    # normalize current column\n",
        "  Q1_t = np.transpose(Q)\n",
        "  # c1 = Q^T * b\n",
        "  c1 = np.dot(Q1_t, b)\n",
        "  x = back_substituion(R, c1)\n",
        "  return x\n",
        "\n",
        "\n",
        "# Modified Gram-Schmidt with augmented matrix \n",
        "# Even with the modified Gram-Schmidt procedure, cancellation can still \n",
        "# occur when components in one vector are subtracted from another, leading \n",
        "# to a significant loss of orthogonality among the columns of Q1 when A is \n",
        "# ill-conditioned, though the loss is much less severe than with classical Gram-Schmidt.\n",
        "# For this reason, when using modified Gram-Schmidt to solve a linear least squares problem Ax = b, \n",
        "# it is not advisable to use the computed Q1 explicitly to compute the transformed right-hand \n",
        "# side c = Q1^T * b. Instead, it is better numerically to treat the right-hand-side vector b \n",
        "# as an (n + 1)-st column, using modified Gram-Schmidt to compute the reduced QR factorization of \n",
        "# the resulting m Ã— (n + 1) augmented matrix.\n",
        "def lsq_v6_augmented_matrix_MGS(A, b):\n",
        "  A = A.reshape((len(A), len(A[0])))\n",
        "  b = b.reshape((len(b), 1))\n",
        "  new_A = np.zeros((len(A), len(A[0]) + 1), dtype=float)\n",
        "  new_A[0:, 0:len(A[0])] = A\n",
        "  new_A[0:, len(A[0]):] = b\n",
        "  m = new_A.shape[0]\n",
        "  n = new_A.shape[1]\n",
        "  # Create two matrix Q and R\n",
        "  R = np.zeros((n, n), dtype=float)\n",
        "  Q = np.zeros((m, n), dtype=float)\n",
        "  for k in range(0, n):\n",
        "    R[k][k] = np.linalg.norm(new_A[0:, k], ord=2)\n",
        "    if R[k][k] == 0:\n",
        "      break\n",
        "    Q[0:, k] = new_A[0:, k] /  R[k][k]\n",
        "    for j in range(k + 1, n):\n",
        "      Qk_t = np.transpose(Q[0:, k])\n",
        "      R[k][j] = np.dot(Qk_t, new_A[0:, j])\n",
        "      new_A[0:, j] = new_A[0:, j] - R[k][j] * Q[0:, k]\n",
        "  new_R = np.zeros((n - 1, n - 1), dtype=float)\n",
        "  new_R = R[0:n-1, 0:n-1]\n",
        "  new_c1 = np.zeros((n - 1, 1), dtype=float)\n",
        "  new_c1 = R[0:n-1, n-1]\n",
        "  new_c1 = new_c1.reshape((n - 1, 1))\n",
        "  x = back_substituion(new_R, new_c1)\n",
        "  return x\n",
        "  \n",
        "\n",
        "# Householder\n",
        "# Each Householder transformation must be applied to the remaining unreduced \n",
        "# portion of the matrix, but it will not affect the prior columns already reduced, \n",
        "# and hence the zeros are preserved through successive transformations.\n",
        "# Moreover, unnecessary overflow or underflow can be avoided by appropriate scaling.\n",
        "def lsq_v2_householder(A, b):\n",
        "  b = b.reshape((len(b), 1))\n",
        "  m = A.shape[0]\n",
        "  n = A.shape[1]\n",
        "  for k in range(0, n):      # loop over columns\n",
        "    # Ak = -sign(Akk) * sqrt(Akk^2 + ... + Amk^2)\n",
        "    alpha_k = (-1) * np.sign(A[k][k]) * np.linalg.norm(A[k:, k], ord=2)    # compute Householder vector for current col\n",
        "    # Vk = [0, ..., 0, Akk, ..., Amk]^T - Akek\n",
        "    Vk = np.zeros((m, 1))\n",
        "    Vk = Vk.reshape((m, 1))\n",
        "    t = np.zeros((m, 1))\n",
        "    t = t.reshape((m, 1))\n",
        "    e_k = np.zeros(shape = (m, 1))\n",
        "    e_k = e_k.reshape((m, 1))\n",
        "    e_k[k][0] = 1\n",
        "    t[k:, 0] = A[k:, k]\n",
        "    Vk = t - alpha_k * e_k\n",
        "    V_k_t = np.transpose(Vk)\n",
        "    # beta_k = Vk^T * Vk\n",
        "    beta_k = np.dot(V_k_t, Vk)\n",
        "    # if beta_k = 0 then\n",
        "    if beta_k == 0:      # skip current column if it is already 0\n",
        "      # continue with next k\n",
        "      continue\n",
        "    for j in range(k, n):    # apply transformation to remaining submatrix\n",
        "      Aa = np.zeros((m, 1))\n",
        "      Aa = A[0:, j]\n",
        "      Aa = Aa.reshape((m, 1))\n",
        "      gamma_j = np.dot(V_k_t, Aa)\n",
        "      # Aj = Aj - (2 * gamma_j / beta_k) * Vk\n",
        "      A[0:, j] = A[0:, j] - (2 * gamma_j[0][0] / beta_k[0][0]) * Vk[0:, 0]\n",
        "    # we need to do some transformation to b to get Hb\n",
        "    Bb = np.zeros((m, 1))\n",
        "    Bb = b[0:, 0]\n",
        "    Bb = Bb.reshape((m, 1))\n",
        "    gamma_j = np.dot(V_k_t, Bb)\n",
        "    b[0:, 0] = b[0:, 0] - (2 * gamma_j[0][0] / beta_k[0][0]) * Vk[0:, 0]\n",
        "    \n",
        "  R = np.zeros((n, n))\n",
        "  R[0:, 0:] = A[0:n, 0:]\n",
        "  c1 = np.zeros((n, 1))  \n",
        "  c1 = c1.reshape(n, 1)\n",
        "  c1[0:, 0] = b[0:n, 0]\n",
        "  x = back_substituion(R, c1)\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "b = np.ones(10)\n",
        "A = hilbert(10)\n",
        "A = np.delete(A, 5, 1)\n",
        "A = np.delete(A, 4, 1)\n",
        "\n",
        "print('Condition number of A: ', np.linalg.norm(A) * np.linalg.norm(np.linalg.pinv(A)))\n",
        "print(\"\\n\")\n",
        "# Use this test instance while developing the algorithms. \n",
        "# This instance is much easier to debug than the ill-conditioned problem above. \n",
        "A_test = np.array([[1, 2, 2], [4, 4, 2], [4, 6, 4]], dtype=float)\n",
        "b_test = np.array([3, 6, 10], dtype=float)\n",
        "\n",
        "#A = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, 1, 0], [-1, 0, 1], [0, -1, 1]], dtype=float)\n",
        "#b = np.array([1237, 1941, 2417, 711, 1177, 475], dtype=float)\n",
        "\n",
        "#A = np.array([[1, 2, 2], [4, 4, 2], [4, 6, 4]], dtype=float)\n",
        "#b = np.array([3, 6, 10], dtype=float)\n",
        "\n",
        "\n",
        "x3 = lsq_v3_normal(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x3)\n",
        "print('Normal equation residual norm: ', np.linalg.norm(A.dot(x3) - b.reshape((len(b), 1)), ord = 2))\n",
        "\n",
        "x5 = lsq_v5_CGS(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x5)\n",
        "print('Classic Gram-Schmidt residual norm: ', np.linalg.norm(A.dot(x5) - b.reshape((len(b), 1)), ord = 2))\n",
        "\n",
        "x1 = lsq_v1_MGS(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x1)\n",
        "print('Modified Gram-Schmidt residual norm: ', np.linalg.norm(A.dot(x1) - b.reshape((len(b), 1)), ord = 2))\n",
        "\n",
        "x6 = lsq_v6_augmented_matrix_MGS(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x6)\n",
        "print('Modified Gram-Schmidt with augmented matrix residual norm: ', np.linalg.norm(A.dot(x6) - b.reshape((len(b), 1)), ord = 2))\n",
        "\n",
        "x2 = lsq_v2_householder(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x2)\n",
        "print('Householder residual norm: ', np.linalg.norm(A.dot(x2) - b.reshape((len(b), 1)), ord = 2))\n",
        "\n",
        "x4 = lsq_v4_library(copy.deepcopy(A), copy.deepcopy(b))\n",
        "#print(x4)\n",
        "print('Library method residual norm: ', np.linalg.norm(A.dot(x4) - b, ord = 2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Condition number of A:  3796554168.541331\n",
            "\n",
            "\n",
            "Normal equation residual norm:  0.0001375497387011829\n",
            "Classic Gram-Schmidt residual norm:  0.0010355321472345004\n",
            "Modified Gram-Schmidt residual norm:  1.3094886573253993e-05\n",
            "Modified Gram-Schmidt with augmented matrix residual norm:  1.3094846972702966e-05\n",
            "Householder residual norm:  1.3094845961453556e-05\n",
            "Library method residual norm:  1.3094846687100912e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}